{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8379af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import jarque_bera, pearsonr,norm,stats,ks_2samp\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold, KFold, GridSearchCV, cross_val_predict, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer,accuracy_score, precision_score, recall_score, classification_report, r2_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from arch.unitroot import PhillipsPerron\n",
    "from scipy.linalg import orth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from scipy.special import gamma\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9de4e",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7067ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new Actual data.csv')\n",
    "original_data = data.copy()\n",
    "data.shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa67c5",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691565e",
   "metadata": {},
   "source": [
    "Conditional Tabular Generative Adversarial Network(CTGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle compositional data (clay, silt, sand) with ILR transformation\n",
    "def ilr_transform(data, comp_columns=['clay (%)', 'silt (%)', 'sand (%)']):\n",
    "    # Replace zeros with small value to avoid log(0)\n",
    "    data[comp_columns] = data[comp_columns].replace(0, 1e-6)\n",
    "    comp_data = data[comp_columns].values\n",
    "    ilr_data = np.log(comp_data[:, :-1] / comp_data[:, -1][:, np.newaxis]) @ orth(np.eye(len(comp_columns) - 1))\n",
    "    ilr_columns = [f'ilr_{i+1}' for i in range(len(comp_columns) - 1)]\n",
    "    ilr_df = pd.DataFrame(ilr_data, columns=ilr_columns, index=data.index)\n",
    "    return ilr_df\n",
    "\n",
    "def inverse_ilr_transform(ilr_data, comp_columns=['clay (%)', 'silt (%)', 'sand (%)']):\n",
    "    # Inverse ILR transformation to recover compositional data\n",
    "    ilr_data = ilr_data.values\n",
    "    n_comps = len(comp_columns)\n",
    "    ilr_mat = orth(np.eye(n_comps - 1))\n",
    "    exp_data = np.exp(ilr_data @ ilr_mat.T)\n",
    "    comp_data = exp_data / (1 + exp_data.sum(axis=1))[:, np.newaxis]\n",
    "    comp_data = np.hstack((comp_data, 1 - comp_data.sum(axis=1)[:, np.newaxis]))\n",
    "    # Ensure sum to 100%\n",
    "    comp_data = comp_data / comp_data.sum(axis=1)[:, np.newaxis] * 100\n",
    "    return pd.DataFrame(comp_data, columns=comp_columns)\n",
    "\n",
    "# Apply ILR transformation to compositional columns\n",
    "ilr_df = ilr_transform(data)\n",
    "data = pd.concat([data.drop(columns=['clay (%)', 'silt (%)', 'sand (%)']), ilr_df], axis=1)\n",
    "\n",
    "# Update numerical columns list to include ILR columns\n",
    "numerical_columns = [col for col in data.columns if col not in ['clay (%)', 'silt (%)', 'sand (%)','station', 'sample number']] + ['ilr_1', 'ilr_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be11948",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctgan = data.copy()\n",
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "data_ctgan[numerical_columns] = scaler.fit_transform(data_ctgan[numerical_columns])\n",
    "\n",
    "# Drop identifier columns for CTGAN training\n",
    "data_for_ctgan = data_ctgan.drop(columns=['station', 'sample number'])\n",
    "\n",
    "# Create metadata for CTGANSynthesizer\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data_for_ctgan)\n",
    "# Ensure all columns are treated as numerical\n",
    "for column in data_for_ctgan.columns:\n",
    "    metadata.update_column(column_name=column, sdtype='numerical')\n",
    "# Initialize and train CTGAN\n",
    "model = CTGANSynthesizer(\n",
    "    metadata=metadata,\n",
    "    epochs=500,\n",
    "    batch_size=100,\n",
    "    verbose=True\n",
    ")\n",
    "model.fit(data_for_ctgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a15b9",
   "metadata": {},
   "source": [
    "GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11df98b",
   "metadata": {},
   "source": [
    "Training Gaussian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('new Actual data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'new Actual data.csv' not found. Please ensure the file exists.\")\n",
    "    raise\n",
    "\n",
    "# Define the expected column order from the original table\n",
    "original_columns = [\n",
    "    'station', 'sample number', 'longitude (° E)', 'latitude (°N)', \n",
    "    'Cr(mg/kg)', 'Cu(mg/kg)', 'Zn(mg/kg)', 'As(mg/kg)', 'Cd(mg/kg)', \n",
    "    'Pb(mg/kg)', 'Hg(mg/kg)', 'Ni(mg/kg)', 'Al2O3 (%)', \n",
    "    'clay (%)', 'silt (%)', 'sand (%)', 'mean grain size(Mz, Φ)', 'water depth (m)'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Check for missing columns\n",
    "missing_cols = [col for col in original_columns if col not in data.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: Missing columns in loaded data: {missing_cols}\")\n",
    "    print(\"Please verify the column names in your CSV file. They must match exactly:\")\n",
    "    print(original_columns)\n",
    "    raise KeyError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "# ILR transformation functions\n",
    "def ilr_transform(data, comp_columns=['clay (%)', 'silt (%)', 'sand (%)']):\n",
    "    # Check if compositional columns exist\n",
    "    missing_comp_cols = [col for col in comp_columns if col not in data.columns]\n",
    "    if missing_comp_cols:\n",
    "        print(f\"Error: Compositional columns {missing_comp_cols} not found in DataFrame. Available columns: {list(data.columns)}\")\n",
    "        raise KeyError(f\"Compositional columns {missing_comp_cols} not found\")\n",
    "    \n",
    "    # Replace zeros with small value to avoid log(0)\n",
    "    data[comp_columns] = data[comp_columns].replace(0, 1e-6)\n",
    "    comp_data = data[comp_columns].values\n",
    "    ilr_data = np.log(comp_data[:, :-1] / comp_data[:, -1][:, np.newaxis]) @ orth(np.eye(len(comp_columns) - 1))\n",
    "    ilr_columns = [f'ilr_{i+1}' for i in range(len(comp_columns) - 1)]\n",
    "    ilr_df = pd.DataFrame(ilr_data, columns=ilr_columns, index=data.index)\n",
    "    return ilr_df\n",
    "\n",
    "def inverse_ilr_transform(ilr_data, comp_columns=['clay (%)', 'silt (%)', 'sand (%)']):\n",
    "    # Inverse ILR transformation to recover compositional data\n",
    "    ilr_data = ilr_data.values\n",
    "    n_comps = len(comp_columns)\n",
    "    ilr_mat = orth(np.eye(n_comps - 1))\n",
    "    exp_data = np.exp(ilr_data @ ilr_mat.T)\n",
    "    comp_data = exp_data / (1 + exp_data.sum(axis=1))[:, np.newaxis]\n",
    "    comp_data = np.hstack((comp_data, 1 - comp_data.sum(axis=1)[:, np.newaxis]))\n",
    "    # Ensure sum to 100%\n",
    "    comp_data = comp_data / comp_data.sum(axis=1)[:, np.newaxis] * 100\n",
    "    return pd.DataFrame(comp_data, columns=comp_columns)\n",
    "def prepare_data_for_gaussian_synthesis(original_data):\n",
    "   \n",
    "    data = original_data.copy()\n",
    "    \n",
    "    # Apply ILR transformation to compositional columns\n",
    "    ilr_df = ilr_transform(data)\n",
    "    data = pd.concat([data.drop(columns=['clay (%)', 'silt (%)', 'sand (%)']), ilr_df], axis=1)\n",
    "    \n",
    "    # Define numerical columns\n",
    "    numerical_columns = [\n",
    "        'longitude (° E)', 'latitude (°N)', 'Cr(mg/kg)', 'Cu(mg/kg)', \n",
    "        'Zn(mg/kg)', 'As(mg/kg)', 'Cd(mg/kg)', 'Pb(mg/kg)', 'Hg(mg/kg)', \n",
    "        'Ni(mg/kg)', 'Al2O3 (%)', 'mean grain size(Mz, Φ)', 'water depth (m)', \n",
    "        'ilr_1', 'ilr_2'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    existing_numerical_columns = [col for col in numerical_columns if col in data.columns]\n",
    "    \n",
    "    # Handle any missing values\n",
    "    data = data.fillna(data.median(numeric_only=True))\n",
    "    \n",
    "    # Normalize numerical columns\n",
    "    scaler = MinMaxScaler()\n",
    "    data[existing_numerical_columns] = scaler.fit_transform(data[existing_numerical_columns])\n",
    "    \n",
    "    return data, scaler, existing_numerical_columns\n",
    "\n",
    "\n",
    "def train_gaussian_model(data):\n",
    "    # Remove identifier columns for training\n",
    "    identifier_columns = ['station', 'sample number']\n",
    "    data_for_training = data.copy()\n",
    "    \n",
    "    # Drop identifier columns if they exist\n",
    "    for col in identifier_columns:\n",
    "        if col in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=[col])\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = SingleTableMetadata()\n",
    "    \n",
    "    # Add all columns as numerical\n",
    "    for column in data_for_training.columns:\n",
    "        metadata.add_column(column_name=column, sdtype='numerical')\n",
    "    \n",
    "    # Initialize the synthesizer\n",
    "    synthesizer = GaussianCopulaSynthesizer(\n",
    "        metadata=metadata,\n",
    "        numerical_distributions={\n",
    "            'longitude (° E)': 'truncnorm',\n",
    "            'latitude (°N)': 'truncnorm',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Fit the model\n",
    "    synthesizer.fit(data_for_training)\n",
    "    \n",
    "    \n",
    "    return synthesizer, data_for_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa66d9c",
   "metadata": {},
   "source": [
    "Generation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_batch_complete(synthesizer, scaler, numerical_columns, \n",
    "                                    batch_size=300, start_station=164):\n",
    "    # Generate synthetic samples\n",
    "    synthetic_batch = synthesizer.sample(num_rows=batch_size)\n",
    "    \n",
    "    # Reverse normalization for numerical columns\n",
    "    existing_numerical_columns = [col for col in numerical_columns if col in synthetic_batch.columns]\n",
    "    if existing_numerical_columns:\n",
    "        synthetic_batch[existing_numerical_columns] = scaler.inverse_transform(\n",
    "            synthetic_batch[existing_numerical_columns]\n",
    "        )\n",
    "    \n",
    "    # Inverse ILR transformation to recover compositional data\n",
    "    if 'ilr_1' in synthetic_batch.columns and 'ilr_2' in synthetic_batch.columns:\n",
    "        ilr_data = synthetic_batch[['ilr_1', 'ilr_2']]\n",
    "        comp_data = inverse_ilr_transform(ilr_data)\n",
    "        synthetic_batch = pd.concat([synthetic_batch.drop(columns=['ilr_1', 'ilr_2']), comp_data], axis=1)\n",
    "    \n",
    "    # Add station and sample number columns\n",
    "    end_station = start_station + batch_size\n",
    "    synthetic_batch['station'] = range(start_station, end_station)\n",
    "    synthetic_batch['sample number'] = [f'BC{str(i).zfill(3)}' for i in range(start_station, end_station)]\n",
    "    \n",
    "    # Ensure all original columns are present in the correct order\n",
    "    for col in original_columns:\n",
    "        if col not in synthetic_batch.columns:\n",
    "            if col in ['station', 'sample number']:\n",
    "                continue  # Already handled\n",
    "            synthetic_batch[col] = 0  # Default value for missing columns\n",
    "    \n",
    "    # Reorder columns to match original table\n",
    "    synthetic_batch = synthetic_batch[original_columns]\n",
    "    \n",
    "    # Round numerical values for better presentation\n",
    "    float_columns = synthetic_batch.select_dtypes(include=[np.float64, np.float32]).columns\n",
    "    for col in float_columns:\n",
    "        if any(keyword in col.lower() for keyword in ['longitude', 'latitude', 'depth', 'grain', 'clay', 'silt', 'sand']):\n",
    "            synthetic_batch[col] = synthetic_batch[col].round(2)\n",
    "        else:\n",
    "            synthetic_batch[col] = synthetic_batch[col].round(3)\n",
    "    \n",
    "    return synthetic_batch\n",
    "\n",
    "def generate_complete_synthetic_dataset(original_data, total_samples=900, batch_size=300, \n",
    "                                      output_filename=\"gaussian_synthetic_complete.csv\"):\n",
    "    prepared_data, scaler, numerical_columns = prepare_data_for_gaussian_synthesis(original_data)\n",
    "    \n",
    "    synthesizer, training_data = train_gaussian_model(prepared_data)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    synthetic_batches = []\n",
    "    current_station = 164\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        current_batch_size = min(batch_size, total_samples - i * batch_size)\n",
    "        \n",
    "        try:\n",
    "            batch = generate_synthetic_batch_complete(\n",
    "                synthesizer=synthesizer,\n",
    "                scaler=scaler,\n",
    "                numerical_columns=numerical_columns,\n",
    "                batch_size=current_batch_size,\n",
    "                start_station=current_station\n",
    "            )\n",
    "            \n",
    "            # Verify batch has correct columns and order\n",
    "            if list(batch.columns) != original_columns:\n",
    "                print(f\"Warning: Batch {i+1} columns do not match original order. Reordering...\")\n",
    "                batch = batch[original_columns]\n",
    "            \n",
    "            synthetic_batches.append(batch)\n",
    "            current_station += current_batch_size\n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating batch {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not synthetic_batches:\n",
    "        print(\"ERROR: No synthetic batches were generated!\")\n",
    "        return None\n",
    "    \n",
    "    all_synthetic = pd.concat(synthetic_batches, ignore_index=True)\n",
    "    \n",
    "    # Ensure all original columns are present in synthetic data\n",
    "    for col in original_columns:\n",
    "        if col not in all_synthetic.columns:\n",
    "            all_synthetic[col] = 0  # Default value for missing columns\n",
    "    \n",
    "    # Reorder synthetic data columns to match original\n",
    "    all_synthetic = all_synthetic[original_columns]\n",
    "    \n",
    "    # Ensure original data has all columns in correct order\n",
    "    original_data = original_data[original_columns]\n",
    "    \n",
    "    # Combine datasets\n",
    "    complete_dataset = pd.concat([original_data, all_synthetic], ignore_index=True)\n",
    "    \n",
    "    # Verify final column order\n",
    "    if list(complete_dataset.columns) != original_columns:\n",
    "        print(\"Warning: Final dataset columns do not match original order. Reordering...\")\n",
    "        complete_dataset = complete_dataset[original_columns]\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    print(\"Step 6: Saving results...\")\n",
    "    complete_dataset.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== GENERATION COMPLETE ===\")\n",
    "    print(f\"Original data shape: {original_data.shape}\")\n",
    "    print(f\"Synthetic data shape: {all_synthetic.shape}\")\n",
    "    print(f\"Complete dataset shape: {complete_dataset.shape}\")\n",
    "    print(f\"Output saved to: {output_filename}\")\n",
    "    \n",
    "    return complete_dataset\n",
    "\n",
    "# Run the pipeline\n",
    "result = generate_complete_synthetic_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12311e",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_data = pd.read_csv(\"gaussian_synthetic_complete.csv\")\n",
    "ctgan_data = pd.read_csv(\"ctgan.csv\")\n",
    "original_data = pd.read_csv('new Actual data.csv')\n",
    "\n",
    "numerical_cols = [\n",
    "        'longitude (° E)', 'latitude (°N)', 'Cr(mg/kg)', 'Cu(mg/kg)', \n",
    "        'Zn(mg/kg)', 'As(mg/kg)', 'Cd(mg/kg)', 'Pb(mg/kg)', 'Hg(mg/kg)', \n",
    "        'Ni(mg/kg)', 'Al2O3 (%)', 'mean grain size(Mz, Φ)', 'water depth (m)', \n",
    "    ] \n",
    "original_numeric = original_data[numerical_cols]\n",
    "ctgan_numeric = ctgan_data[numerical_cols]\n",
    "gaussian_numeric = gaussian_data[numerical_cols]\n",
    "\n",
    "# Utility Metric: RMSE using Linear Regression\n",
    "# Prepare data (using all features except target to predict As and Zn)\n",
    "X_original = original_numeric.drop(['As(mg/kg)', 'Zn(mg/kg)'], axis=1)\n",
    "y_original_As = original_numeric['As(mg/kg)']\n",
    "y_original_Zn = original_numeric['Zn(mg/kg)']\n",
    "\n",
    "# Function to calculate RMSE for utility\n",
    "def calculate_rmse(original_X, original_y, synthetic_X, synthetic_y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(synthetic_X, synthetic_y)\n",
    "    y_pred = model.predict(original_X)\n",
    "    rmse = np.sqrt(mean_squared_error(original_y, y_pred))\n",
    "    return rmse\n",
    "\n",
    "# RMSE for CTGAN\n",
    "X_ctgan = ctgan_numeric.drop(['As(mg/kg)', 'Zn(mg/kg)'], axis=1)\n",
    "y_ctgan_As = ctgan_numeric['As(mg/kg)']\n",
    "y_ctgan_Zn = ctgan_numeric['Zn(mg/kg)']\n",
    "rmse_ctgan_As = calculate_rmse(X_original, y_original_As, X_ctgan, y_ctgan_As)\n",
    "rmse_ctgan_Zn = calculate_rmse(X_original, y_original_Zn, X_ctgan, y_ctgan_Zn)\n",
    "\n",
    "# RMSE for GaussianCopula\n",
    "X_gaussian = gaussian_numeric.drop(['As(mg/kg)', 'Zn(mg/kg)'], axis=1)\n",
    "y_gaussian_As = gaussian_numeric['As(mg/kg)']\n",
    "y_gaussian_Zn = gaussian_numeric['Zn(mg/kg)']\n",
    "rmse_gaussian_As = calculate_rmse(X_original, y_original_As, X_gaussian, y_gaussian_As)\n",
    "rmse_gaussian_Zn = calculate_rmse(X_original, y_original_Zn, X_gaussian, y_gaussian_Zn)\n",
    "\n",
    "# Print RMSE results\n",
    "print(\"\\nRMSE Results for Arsenic (As) and Zinc (Zn):\")\n",
    "print(f\"CTGAN RMSE - As: {rmse_ctgan_As:.3f}, Zn: {rmse_ctgan_Zn:.3f}\")\n",
    "print(f\"GaussianCopula RMSE - As: {rmse_gaussian_As:.3f}, Zn: {rmse_gaussian_Zn:.3f}\")\n",
    "\n",
    "#mean and standard deviation  for original\n",
    "mean_org = original_data.mean(numeric_only=True).round(3)\n",
    "std_org  = original_data.std(numeric_only=True).round(3)\n",
    "\n",
    "#mean and standard deviation  for cleaned data \n",
    "mean_gaussian = gaussian_data.mean(numeric_only=True).round(3)\n",
    "std_gaussian  = gaussian_data.std(numeric_only=True).round(3)\n",
    "\n",
    "\n",
    "#mean and standard deviation  for cleaned data \n",
    "mean_ctgan = ctgan_data.mean(numeric_only=True).round(3)\n",
    "std_ctgan  = ctgan_data.std(numeric_only=True).round(3)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "summary_df = pd.concat([mean_org, mean_gaussian, mean_ctgan, std_org,std_gaussian, std_ctgan], axis=1)      \\\n",
    "               .reset_index()                      \\\n",
    "               .rename(columns={\n",
    "                   'index': 'Column',\n",
    "                   0: 'Mean (Original)',\n",
    "                   1: 'Mean (Gaussian)',\n",
    "                   2: \"Mean(Ctgan)\",\n",
    "                   3: 'std (original)',\n",
    "                   4: 'std(Gaussian)',\n",
    "                   5: \"std(ctgan)\"\n",
    "                   \n",
    "               })\n",
    "summary_df.to_csv(\"Data gen comparison.csv\", index=False)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7915a3",
   "metadata": {},
   "source": [
    "Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets and their properties\n",
    "datasets = [\n",
    "    {'data': original_data, 'label': 'Original', 'color': 'blue'},\n",
    "    {'data': gaussian_data, 'label': 'GaussianCopula', 'color': 'green'},\n",
    "    {'data': ctgan_data, 'label': 'CTGAN', 'color': 'orange'}\n",
    "]\n",
    "variables = ['Zn(mg/kg)', 'As(mg/kg)']\n",
    "\n",
    "# Create figure with subplots (3 rows per variable, 2 columns for Zn and As)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 12), sharex='col')\n",
    "\n",
    "# Plot histograms for each variable\n",
    "for col_idx, var in enumerate(variables):\n",
    "    for row_idx, dataset in enumerate(datasets):\n",
    "        sns.histplot(\n",
    "            data=dataset['data'],\n",
    "            x=var,\n",
    "            color=dataset['color'],\n",
    "            label=dataset['label'],\n",
    "            alpha=0.6,\n",
    "            stat='density',\n",
    "            ax=axes[row_idx, col_idx]\n",
    "        )\n",
    "        axes[row_idx, col_idx].set_title(f'Distribution of {var} - {dataset[\"label\"]} Data')\n",
    "        axes[row_idx, col_idx].set_xlabel(var)\n",
    "        axes[row_idx, col_idx].set_ylabel('Density')\n",
    "        axes[row_idx, col_idx].legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab874ec6",
   "metadata": {},
   "source": [
    "Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe104444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric columns for analysis\n",
    "data = gaussian_data.copy()\n",
    "numeric_data = data.iloc[:,2:]\n",
    "numeric_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a926574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_df = numeric_data.corr()\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Heavy Metals and Other Variables\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Correlation with Arsenic\n",
    "as_corr = corr_df[['As(mg/kg)']].sort_values(by='As(mg/kg)', ascending=False)\n",
    "\n",
    "# Correlation with Zinc\n",
    "zn_corr = corr_df[['Zn(mg/kg)']].sort_values(by='Zn(mg/kg)', ascending=False)\n",
    "\n",
    "# Display the two tables\n",
    "print(\"\\nCorrelation with Arsenic (As):\\n\", as_corr)\n",
    "print(\"\\nCorrelation with Zinc (Zn):\\n\", zn_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b01ed",
   "metadata": {},
   "source": [
    "Stationary Test(ADF AND PP Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Function to perform ADF and PP tests and return results\n",
    "def stationary_tests(series, col_name):\n",
    "    result_row = {'Feature': col_name}\n",
    "    \n",
    "    # ADF Test\n",
    "    try:\n",
    "        adf_result = adfuller(series, autolag='AIC')\n",
    "        result_row['ADF_Statistic'] = round(adf_result[0], 4)\n",
    "        result_row['ADF_p-value'] = round(adf_result[1], 4)\n",
    "        result_row['ADF_Result'] = 'Stationary' if adf_result[1] < 0.05 else 'Non-Stationary'\n",
    "    except Exception as e:\n",
    "        result_row['ADF_Statistic'] = None\n",
    "        result_row['ADF_p-value'] = None\n",
    "        result_row['ADF_Result'] = f'Error: {str(e)}'\n",
    "    \n",
    "    # PP Test\n",
    "    try:\n",
    "        pp = PhillipsPerron(series)\n",
    "        result_row['PP_Statistic'] = round(pp.stat, 4)\n",
    "        result_row['PP_p-value'] = round(pp.pvalue, 4)\n",
    "        result_row['PP_Result'] = 'Stationary' if pp.pvalue < 0.05 else 'Non-Stationary'\n",
    "    except Exception as e:\n",
    "        result_row['PP_Statistic'] = None\n",
    "        result_row['PP_p-value'] = None\n",
    "        result_row['PP_Result'] = f'Error: {str(e)}'\n",
    "    \n",
    "    return result_row\n",
    "\n",
    "# Perform tests for each numerical column\n",
    "for col in numeric_data.columns:\n",
    "    series = numeric_data[col].dropna()  # Drop NaNs if any\n",
    "    if len(series) > 1:  # Ensure enough data points\n",
    "        result = stationary_tests(series, col)\n",
    "        results.append(result)\n",
    "    else:\n",
    "        results.append({\n",
    "            'Feature': col,\n",
    "            'ADF_Statistic': None,\n",
    "            'ADF_p-value': None,\n",
    "            'ADF_Result': 'Insufficient data',\n",
    "            'PP_Statistic': None,\n",
    "            'PP_p-value': None,\n",
    "            'PP_Result': 'Insufficient data'\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "results_df = results_df[[\n",
    "    'Feature', \n",
    "    'ADF_Statistic', 'ADF_p-value', \n",
    "    'PP_Statistic', 'PP_p-value', \n",
    "]]\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nStationarity Test Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce9378",
   "metadata": {},
   "source": [
    "Normality test(Jaquera-Bera Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Jarque-Bera test for each column\n",
    "jb_results = []\n",
    "for col in numeric_data.columns:\n",
    "    stat, p_value = jarque_bera(numeric_data[col].dropna())\n",
    "    jb_results.append({\n",
    "        'Variable': col,\n",
    "        'JB Statistic': stat,\n",
    "        'p-value': p_value,\n",
    "        \n",
    "    })\n",
    "# Create DataFrame of results\n",
    "jb_df = pd.DataFrame(jb_results)\n",
    "\n",
    "# Display\n",
    "print(jb_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571b4c8",
   "metadata": {},
   "source": [
    "Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f973cc",
   "metadata": {},
   "source": [
    "Elman Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce06e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate MAPE, avoiding division by zero.\"\"\"\n",
    "    # Handle NaN and infinite values\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true != 0)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0\n",
    "\n",
    "\n",
    "def calculate_nse(y_true, y_pred):\n",
    "    \"\"\"Calculate Nash-Sutcliffe Efficiency (NSE).\"\"\"\n",
    "    # Handle NaN values\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    mean_obs = np.mean(y_true_clean)\n",
    "    ss_res = np.sum((y_true_clean - y_pred_clean) ** 2)\n",
    "    ss_tot = np.sum((y_true_clean - mean_obs) ** 2)\n",
    "    return 1 - (ss_res / ss_tot) if ss_tot != 0 else np.nan\n",
    "\n",
    "\n",
    "def calculate_pbias(y_true, y_pred):\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    return np.sum(y_true_clean - y_pred_clean) / np.sum(y_true_clean) * 100 if np.sum(y_true_clean) != 0 else np.nan\n",
    "\n",
    "\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2))\n",
    "\n",
    "\n",
    "def calculate_pcc(y_true, y_pred):\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if np.sum(mask) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    # Check if there's variance in both arrays\n",
    "    if np.std(y_true_clean) == 0 or np.std(y_pred_clean) == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    try:\n",
    "        pcc, p_value = pearsonr(y_true_clean, y_pred_clean)\n",
    "        return pcc if np.isfinite(pcc) else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def calculate_lag_correlation(y_true, y_pred, lag=1):\n",
    "    \"\"\"Calculate correlation between predictions and lagged true values.\"\"\"\n",
    "    if len(y_true) < lag + 1:\n",
    "        return np.nan\n",
    "    \n",
    "    mask_true = np.isfinite(y_true[lag:])\n",
    "    mask_pred = np.isfinite(y_pred[:-lag])\n",
    "    mask = mask_true & mask_pred\n",
    "    \n",
    "    if np.sum(mask) < 2:\n",
    "        return np.nan\n",
    "        \n",
    "    try:\n",
    "        return pearsonr(y_true[lag:][mask], y_pred[:-lag][mask])[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def calculate_morans_i(y_pred, coords, k=3):\n",
    "    \"\"\"Calculate Moran's I for spatial autocorrelation.\"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = np.isfinite(y_pred)\n",
    "    if np.sum(mask) < k + 1:\n",
    "        return np.nan\n",
    "    \n",
    "    y_pred_clean = y_pred[mask]\n",
    "    coords_clean = coords[mask]\n",
    "    \n",
    "    if len(y_pred_clean) < k + 1:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(k, len(coords_clean)-1)).fit(coords_clean)\n",
    "        distances, indices = nbrs.kneighbors(coords_clean)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        w = 1 / (distances + 1e-8)  # Increased epsilon\n",
    "        w = w / (w.sum(axis=1, keepdims=True) + 1e-8)  # Normalize with safety\n",
    "        \n",
    "        y_pred_clean = y_pred_clean.flatten()\n",
    "        y_mean = np.mean(y_pred_clean)\n",
    "        n = len(y_pred_clean)\n",
    "        \n",
    "        num = 0\n",
    "        den = np.sum((y_pred_clean - y_mean) ** 2)\n",
    "        \n",
    "        if den == 0:\n",
    "            return np.nan\n",
    "            \n",
    "        for i in range(n):\n",
    "            for j in range(min(k, w.shape[1])):\n",
    "                if indices[i, j] < len(y_pred_clean):  # Safety check\n",
    "                    num += w[i, j] * (y_pred_clean[i] - y_mean) * (y_pred_clean[indices[i, j]] - y_mean)\n",
    "        \n",
    "        w_sum = np.sum(w)\n",
    "        return (n / w_sum) * (num / den) if w_sum != 0 and den != 0 else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def create_weighted_knn_sequences(X_coords, X_features, y, k=3):\n",
    "    \"\"\"Create KNN sequences with distance-based weights.\"\"\"\n",
    "    if len(X_coords) < k:\n",
    "        k = max(1, len(X_coords) - 1)\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_coords)\n",
    "    distances, indices = nbrs.kneighbors(X_coords)\n",
    "    \n",
    "    # Improved weight calculation\n",
    "    weights = 1 / (distances + 1e-8)  # Increased epsilon\n",
    "    weights = weights / (weights.sum(axis=1, keepdims=True) + 1e-8)  # Normalize with safety\n",
    "    \n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_features)):\n",
    "        # Ensure indices are valid\n",
    "        valid_indices = indices[i][indices[i] < len(X_features)]\n",
    "        if len(valid_indices) == 0:\n",
    "            valid_indices = [i]  # Use self if no valid neighbors\n",
    "        \n",
    "        seq = X_features[valid_indices[:k]] * weights[i][:len(valid_indices)][:, None]\n",
    "        X_seq.append(seq)\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "class ImprovedElmanNetwork:\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, lr=0.01, l2_lambda=0.001):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Improved weight initialization (Xavier/Glorot)\n",
    "        limit_xh = np.sqrt(6 / (n_inputs + n_hidden))\n",
    "        limit_ch = np.sqrt(6 / (n_hidden + n_hidden))\n",
    "        limit_hy = np.sqrt(6 / (n_hidden + n_outputs))\n",
    "        \n",
    "        self.W_xh = np.random.uniform(-limit_xh, limit_xh, (n_inputs, n_hidden))\n",
    "        self.W_ch = np.random.uniform(-limit_ch, limit_ch, (n_hidden, n_hidden))\n",
    "        self.b_h = np.zeros((1, n_hidden))\n",
    "        self.W_hy = np.random.uniform(-limit_hy, limit_hy, (n_hidden, n_outputs))\n",
    "        self.b_y = np.zeros((1, n_outputs))\n",
    "\n",
    "        # Adam parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "\n",
    "        # Adam moments\n",
    "        self.m_W_xh = np.zeros_like(self.W_xh)\n",
    "        self.v_W_xh = np.zeros_like(self.W_xh)\n",
    "        self.m_W_ch = np.zeros_like(self.W_ch)\n",
    "        self.v_W_ch = np.zeros_like(self.W_ch)\n",
    "        self.m_b_h = np.zeros_like(self.b_h)\n",
    "        self.v_b_h = np.zeros_like(self.b_h)\n",
    "        self.m_W_hy = np.zeros_like(self.W_hy)\n",
    "        self.v_W_hy = np.zeros_like(self.W_hy)\n",
    "        self.m_b_y = np.zeros_like(self.b_y)\n",
    "        self.v_b_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        # Context layer\n",
    "        self.context = np.zeros((1, n_hidden))\n",
    "\n",
    "    def reset_context(self):\n",
    "        self.context = np.zeros((1, self.n_hidden))\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"Forward pass for a sequence.\"\"\"\n",
    "        h_states = []\n",
    "        self.reset_context()\n",
    "        \n",
    "        for t in range(x_seq.shape[0]):\n",
    "            x_t = x_seq[t:t+1]\n",
    "            h_net = np.dot(x_t, self.W_xh) + np.dot(self.context, self.W_ch) + self.b_h\n",
    "            \n",
    "            # Use tanh activation with gradient clipping\n",
    "            h = np.tanh(np.clip(h_net, -10, 10))\n",
    "            h_states.append(h)\n",
    "            self.context = h\n",
    "            \n",
    "        # Final output\n",
    "        y_pred = np.dot(h_states[-1], self.W_hy) + self.b_y\n",
    "        return y_pred, h_states\n",
    "\n",
    "    def backward(self, x_seq, h_states, y_pred, y_true):\n",
    "        \"\"\"Backward pass with BPTT for sequence.\"\"\"\n",
    "        error = y_pred - y_true\n",
    "        \n",
    "        # Check for NaN in error\n",
    "        if np.any(np.isnan(error)) or np.any(np.isinf(error)):\n",
    "            return float('inf')\n",
    "            \n",
    "        loss = np.mean(error ** 2) + self.l2_lambda * (\n",
    "            np.sum(self.W_xh ** 2) + np.sum(self.W_ch ** 2) + np.sum(self.W_hy ** 2)\n",
    "        )\n",
    "\n",
    "        # Output layer gradients\n",
    "        dW_hy = np.dot(h_states[-1].T, error) / x_seq.shape[0] + 2 * self.l2_lambda * self.W_hy\n",
    "        db_y = np.mean(error, axis=0, keepdims=True)\n",
    "        dh = np.dot(error, self.W_hy.T) * (1.0 - h_states[-1] ** 2)\n",
    "\n",
    "        # Recurrent gradients (BPTT)\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_ch = np.zeros_like(self.W_ch)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        \n",
    "        for t in reversed(range(x_seq.shape[0])):\n",
    "            dW_xh += np.dot(x_seq[t:t+1].T, dh) / x_seq.shape[0]\n",
    "            prev_h = h_states[t-1] if t > 0 else np.zeros_like(self.context)\n",
    "            dW_ch += np.dot(prev_h.T, dh) / x_seq.shape[0]\n",
    "            db_h += np.mean(dh, axis=0, keepdims=True)\n",
    "            \n",
    "            if t > 0:\n",
    "                dh = np.dot(dh, self.W_ch.T) * (1.0 - h_states[t-1] ** 2)\n",
    "                # Gradient clipping for vanishing gradient\n",
    "                dh = np.clip(dh, -1, 1)\n",
    "\n",
    "        # Add L2 regularization to weight gradients\n",
    "        dW_xh += 2 * self.l2_lambda * self.W_xh\n",
    "        dW_ch += 2 * self.l2_lambda * self.W_ch\n",
    "\n",
    "        # Improved gradient clipping\n",
    "        max_norm = 5.0  # Reduced from 12.0\n",
    "        grads = [dW_xh, dW_ch, db_h, dW_hy, db_y]\n",
    "        \n",
    "        # Check for NaN gradients\n",
    "        for i, grad in enumerate(grads):\n",
    "            if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
    "                print(f\"NaN/Inf gradient detected in gradient {i}\")\n",
    "                return float('inf')\n",
    "        \n",
    "        norm = np.sqrt(sum(np.sum(g ** 2) for g in grads))\n",
    "        if norm > max_norm:\n",
    "            scale = max_norm / norm\n",
    "            grads = [g * scale for g in grads]\n",
    "        \n",
    "        dW_xh, dW_ch, db_h, dW_hy, db_y = grads\n",
    "\n",
    "        # Adam update with improved numerical stability\n",
    "        self.t += 1\n",
    "        params = [\n",
    "            (dW_xh, 'W_xh'),\n",
    "            (dW_ch, 'W_ch'),\n",
    "            (db_h, 'b_h'),\n",
    "            (dW_hy, 'W_hy'),\n",
    "            (db_y, 'b_y')\n",
    "        ]\n",
    "        \n",
    "        for dparam, name in params:\n",
    "            m = getattr(self, f'm_{name}')\n",
    "            v = getattr(self, f'v_{name}')\n",
    "            \n",
    "            m = self.beta1 * m + (1 - self.beta1) * dparam\n",
    "            v = self.beta2 * v + (1 - self.beta2) * (dparam ** 2)\n",
    "            \n",
    "            m_hat = m / (1 - self.beta1 ** self.t)\n",
    "            v_hat = v / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Improved numerical stability\n",
    "            update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            \n",
    "            # Clip updates to prevent exploding\n",
    "            update = np.clip(update, -0.5, 0.5)\n",
    "            \n",
    "            param = getattr(self, name)\n",
    "            param -= update\n",
    "            \n",
    "            # Check for NaN in parameters\n",
    "            if np.any(np.isnan(param)) or np.any(np.isinf(param)):\n",
    "                print(f\"NaN/Inf parameter detected in {name}\")\n",
    "                return float('inf')\n",
    "            \n",
    "            setattr(self, f'm_{name}', m)\n",
    "            setattr(self, f'v_{name}', v)\n",
    "            setattr(self, name, param)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_preprocess_data(target='As(mg/kg)', corr_threshold=0.05, k=3, drop_cols=None, file_path=\"gaussian_synthetic_complete.csv\"):\n",
    "        \"\"\"Load and create weighted KNN sequences with improved preprocessing.\"\"\"\n",
    "        if drop_cols is None:\n",
    "            drop_cols = ['station', 'sample number', 'longitude (° E)', 'latitude (°N)']\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Initial data shape: {df.shape}\")\n",
    "        \n",
    "        # Handle missing values more carefully\n",
    "        df = df.dropna()\n",
    "        print(f\"After dropping NaN: {df.shape}\")\n",
    "\n",
    "        # Store coordinates before any transformations\n",
    "        coords = df[['longitude (° E)', 'latitude (°N)']].values\n",
    "        y = df[target].values\n",
    "        \n",
    "        print(f\"Target {target} - Min: {y.min():.3f}, Max: {y.max():.3f}, Mean: {y.mean():.3f}\")\n",
    "\n",
    "        # Prepare features\n",
    "        X = df.drop([target] + drop_cols, axis=1, errors='ignore')\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        X = X[numeric_cols]\n",
    "        \n",
    "        print(f\"Features before selection: {X.shape[1]}\")\n",
    "\n",
    "        # Improved log transformation - only for highly skewed features\n",
    "        for col in numeric_cols:\n",
    "            if col in X.columns:\n",
    "                skewness = pd.Series(X[col]).skew()\n",
    "                if abs(skewness) > 2 and X[col].min() >= 0:  # Only transform highly skewed data\n",
    "                    X[col] = np.log1p(X[col])\n",
    "\n",
    "        # Feature selection based on correlation\n",
    "        corr = X.corrwith(pd.Series(y)).abs()\n",
    "        selected_features = corr[corr > corr_threshold].index.tolist()\n",
    "        if not selected_features:\n",
    "            # If no features meet threshold, select top features\n",
    "            selected_features = corr.nlargest(min(10, len(corr))).index.tolist()\n",
    "        \n",
    "        X = X[selected_features]\n",
    "        print(f\"Selected features ({len(selected_features)}): {selected_features}\")\n",
    "\n",
    "        # Improved scaling - use StandardScaler for better numerical stability\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Improved target scaling\n",
    "        y_mean, y_std = np.mean(y), np.std(y)\n",
    "        if y_std == 0:\n",
    "            y_std = 1  # Avoid division by zero\n",
    "        y_scaled = (y - y_mean) / y_std\n",
    "        \n",
    "        print(f\"Y scaling - Mean: {y_mean:.3f}, Std: {y_std:.3f}\")\n",
    "        \n",
    "        # Reshape target\n",
    "        y_scaled = y_scaled.reshape(-1, 1)\n",
    "\n",
    "        # Create weighted KNN sequences\n",
    "        X_seq, y_seq = create_weighted_knn_sequences(coords, X_scaled, y_scaled, k=k)\n",
    "        \n",
    "        print(f\"Sequence shape: {X_seq.shape}\")\n",
    "\n",
    "        # Spatial clustering for split\n",
    "        n_clusters = min(20, len(coords) // 5)  # Adjust clusters based on data size\n",
    "        if n_clusters < 3:\n",
    "            n_clusters = 3\n",
    "            \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(coords)\n",
    "        cluster_labels = kmeans.labels_\n",
    "\n",
    "        return X_seq, y_seq, selected_features, y_mean, y_std, coords, cluster_labels, scaler\n",
    "\n",
    "    def train_epoch(self, X_seq, y_seq):\n",
    "        \"\"\"Train one epoch over sequences.\"\"\"\n",
    "        if X_seq.shape[0] != y_seq.shape[0]:\n",
    "            raise ValueError(\"Sequence mismatch\")\n",
    "            \n",
    "        total_loss = 0.0\n",
    "        valid_samples = 0\n",
    "        indices = np.random.permutation(X_seq.shape[0])\n",
    "        \n",
    "        for i in indices:\n",
    "            try:\n",
    "                y_pred, h_states = self.forward(X_seq[i])\n",
    "                loss = self.backward(X_seq[i], h_states, y_pred, y_seq[i:i+1])\n",
    "                \n",
    "                if np.isfinite(loss):\n",
    "                    total_loss += loss\n",
    "                    valid_samples += 1\n",
    "                else:\n",
    "                    print(f\"Invalid loss at sample {i}: {loss}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return total_loss / max(valid_samples, 1)\n",
    "\n",
    "    def predict(self, X_seq):\n",
    "        \"\"\"Predict for sequences.\"\"\"\n",
    "        y_pred = np.zeros((X_seq.shape[0], self.n_outputs))\n",
    "        \n",
    "        for i in range(X_seq.shape[0]):\n",
    "            try:\n",
    "                self.reset_context()\n",
    "                y, _ = self.forward(X_seq[i])\n",
    "                y_pred[i] = y\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting sample {i}: {e}\")\n",
    "                y_pred[i] = np.nan\n",
    "                \n",
    "        return y_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def select_hidden_nodes(n_inputs, n_outputs):\n",
    "        \"\"\"Select reasonable hidden node candidates.\"\"\"\n",
    "        base = max(4, int(np.sqrt(n_inputs * n_outputs)))\n",
    "        return [base, base + 2, base + 4, base + 6, base + 8]\n",
    "\n",
    "\n",
    "def improved_tune_hyperparameters(target='As(mg/kg)', file_path=\"gaussian_synthetic_complete.csv\", max_epochs=50):\n",
    "    \"\"\"Improved hyperparameter tuning.\"\"\"\n",
    "    k_values = [2, 3, 4]\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = {'k': 3, 'n_hidden': 8}\n",
    "    best_model = None\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"\\n=== Testing k={k} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Load data with current k\n",
    "            X_seq, y_seq, selected_features, y_mean, y_std, coords, cluster_labels, scaler = \\\n",
    "                ImprovedElmanNetwork.load_and_preprocess_data(target=target, k=k, file_path=file_path)\n",
    "            \n",
    "            # Cluster-based split\n",
    "            unique_clusters = np.unique(cluster_labels)\n",
    "            n_train = max(1, int(0.7 * len(unique_clusters)))\n",
    "            n_val = max(1, int(0.2 * len(unique_clusters)))\n",
    "            \n",
    "            train_clusters = unique_clusters[:n_train]\n",
    "            val_clusters = unique_clusters[n_train:n_train+n_val]\n",
    "            test_clusters = unique_clusters[n_train+n_val:]\n",
    "            \n",
    "            train_idx = np.isin(cluster_labels, train_clusters)\n",
    "            val_idx = np.isin(cluster_labels, val_clusters)\n",
    "            test_idx = np.isin(cluster_labels, test_clusters)\n",
    "            \n",
    "            X_train_seq = X_seq[train_idx]\n",
    "            y_train_seq = y_seq[train_idx]\n",
    "            X_val_seq = X_seq[val_idx]\n",
    "            y_val_seq = y_seq[val_idx]\n",
    "            \n",
    "            if len(X_train_seq) == 0 or len(X_val_seq) == 0:\n",
    "                print(f\"Insufficient data for k={k}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Train: {len(X_train_seq)}, Val: {len(X_val_seq)}\")\n",
    "            \n",
    "            n_inputs = X_train_seq.shape[2]\n",
    "            n_outputs = 1\n",
    "            \n",
    "            hidden_candidates = ImprovedElmanNetwork.select_hidden_nodes(n_inputs, n_outputs)\n",
    "            \n",
    "            for n_hidden in hidden_candidates:\n",
    "                print(f\"  Testing hidden_nodes={n_hidden}...\")\n",
    "                \n",
    "                # Create model with adjusted learning rate\n",
    "                rnn = ImprovedElmanNetwork(n_inputs, n_hidden, n_outputs, lr=0.005, l2_lambda=0.001)\n",
    "                \n",
    "                best_epoch_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                patience = 15\n",
    "                \n",
    "                for epoch in range(max_epochs):\n",
    "                    train_loss = rnn.train_epoch(X_train_seq, y_train_seq)\n",
    "                    \n",
    "                    if not np.isfinite(train_loss):\n",
    "                        print(f\"    Invalid training loss at epoch {epoch+1}\")\n",
    "                        break\n",
    "                    \n",
    "                    # Validate every 5 epochs\n",
    "                    if (epoch + 1) % 5 == 0:\n",
    "                        val_pred = rnn.predict(X_val_seq)\n",
    "                        val_loss = np.mean((val_pred - y_val_seq) ** 2)\n",
    "                        \n",
    "                        if np.isfinite(val_loss):\n",
    "                            if val_loss < best_epoch_loss:\n",
    "                                best_epoch_loss = val_loss\n",
    "                                patience_counter = 0\n",
    "                            else:\n",
    "                                patience_counter += 1\n",
    "                            \n",
    "                            if patience_counter >= patience:\n",
    "                                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"Invalid validation loss at epoch {epoch+1}\")\n",
    "                            break\n",
    "                \n",
    "                final_val_pred = rnn.predict(X_val_seq)\n",
    "                final_val_loss = np.mean((final_val_pred - y_val_seq) ** 2)\n",
    "                \n",
    "                if np.isfinite(final_val_loss):\n",
    "                    results.append({\n",
    "                        'k': k,\n",
    "                        'n_hidden': n_hidden,\n",
    "                        'val_loss': final_val_loss\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"    Final val loss: {final_val_loss:.6f}\")\n",
    "                    \n",
    "                    if final_val_loss < best_val_loss:\n",
    "                        best_val_loss = final_val_loss\n",
    "                        best_params = {'k': k, 'n_hidden': n_hidden}\n",
    "                        best_model = rnn\n",
    "                else:\n",
    "                    print(f\"Invalid final validation loss\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error with k={k}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nBest parameters: {best_params} (val_loss: {best_val_loss:.6f})\")\n",
    "    return best_params, best_model, results\n",
    "\n",
    "\n",
    "# Example usage and main training loop\n",
    "if __name__ == \"__main__\":\n",
    "    # For Zn prediction\n",
    "    target_metal = 'As(mg/kg)'\n",
    "    print(f\"=== Training for {target_metal} ===\")\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    best_params, best_model, tuning_results = improved_tune_hyperparameters(\n",
    "        target=target_metal, \n",
    "        file_path=\"gaussian_synthetic_complete.csv\"\n",
    "    )\n",
    "    \n",
    "    # Load data with best parameters\n",
    "    X_seq, y_seq, selected_features, y_mean, y_std, coords, cluster_labels, scaler = \\\n",
    "        ImprovedElmanNetwork.load_and_preprocess_data(\n",
    "            target=target_metal, \n",
    "            k=best_params['k'], \n",
    "            file_path=\"gaussian_synthetic_complete.csv\"\n",
    "        )\n",
    "    \n",
    "    # Final data split\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    n_train = max(1, int(0.7 * len(unique_clusters)))\n",
    "    n_val = max(1, int(0.2 * len(unique_clusters)))\n",
    "    \n",
    "    train_clusters = unique_clusters[:n_train]\n",
    "    val_clusters = unique_clusters[n_train:n_train+n_val]\n",
    "    test_clusters = unique_clusters[n_train+n_val:]\n",
    "    \n",
    "    train_idx = np.isin(cluster_labels, train_clusters)\n",
    "    val_idx = np.isin(cluster_labels, val_clusters)\n",
    "    test_idx = np.isin(cluster_labels, test_clusters)\n",
    "    \n",
    "    X_train_seq = X_seq[train_idx]\n",
    "    y_train_seq = y_seq[train_idx]\n",
    "    X_val_seq = X_seq[val_idx]\n",
    "    y_val_seq = y_seq[val_idx]\n",
    "    X_test_seq = X_seq[test_idx]\n",
    "    y_test_seq = y_seq[test_idx]\n",
    "    \n",
    "    print(f\"Final split - Train: {len(X_train_seq)}, Val: {len(X_val_seq)}, Test: {len(X_test_seq)}\")\n",
    "    \n",
    "    # Final model training\n",
    "    n_inputs = X_train_seq.shape[2]\n",
    "    rnn = ImprovedElmanNetwork(n_inputs, best_params['n_hidden'], 1, lr=0.005, l2_lambda=0.001)\n",
    "    \n",
    "    n_epochs = 200\n",
    "    patience = 30\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training with learning rate scheduling\n",
    "    initial_lr = 0.005\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Learning rate decay\n",
    "        if epoch > 0 and epoch % 50 == 0:\n",
    "            rnn.lr *= 0.8\n",
    "        \n",
    "        train_loss = rnn.train_epoch(X_train_seq, y_train_seq)\n",
    "        \n",
    "        if not np.isfinite(train_loss):\n",
    "            print(f\"Training failed at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            val_pred = rnn.predict(X_val_seq)\n",
    "            val_loss = np.mean((val_pred - y_val_seq) ** 2)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {rnn.lr:.6f}\")\n",
    "            \n",
    "            if np.isfinite(val_loss):\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "    # Final predictions and evaluation\n",
    "    y_pred_norm = rnn.predict(X_test_seq)\n",
    "    y_pred = y_pred_norm * y_std + y_mean\n",
    "    y_test_denorm = y_test_seq * y_std + y_mean\n",
    "    \n",
    "    # Apply reasonable clipping based on target\n",
    "    if target_metal == 'Zn(mg/kg)':\n",
    "        y_pred = np.clip(y_pred, 0, 200)  # Reasonable range for Zn\n",
    "    elif target_metal == 'As(mg/kg)':\n",
    "        y_pred = np.clip(y_pred, 0, 50)   # Reasonable range for As\n",
    "    \n",
    "    # Flatten arrays for metric calculations\n",
    "    y_test_flat = y_test_denorm.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    # Calculate all metrics with improved error handling\n",
    "    print(f\"\\n=== Final Results for {target_metal} ===\")\n",
    "    \n",
    "    mape = safe_mape(y_test_flat, y_pred_flat)\n",
    "    nse = calculate_nse(y_test_flat, y_pred_flat)\n",
    "    pbias = calculate_pbias(y_test_flat, y_pred_flat)\n",
    "    mae = calculate_mae(y_test_flat, y_pred_flat)\n",
    "    rmse = calculate_rmse(y_test_flat, y_pred_flat)\n",
    "    pcc = calculate_pcc(y_test_flat, y_pred_flat)\n",
    "    morans_i = calculate_morans_i(y_pred_flat, coords[test_idx], k=3)\n",
    "    \n",
    "    print(f\"Test MAPE: {mape:.2f}%\" if np.isfinite(mape) else \"Test MAPE: N/A\")\n",
    "    print(f\"Test NSE: {nse:.4f}\" if np.isfinite(nse) else \"Test NSE: N/A\")\n",
    "    print(f\"Test PBIAS: {pbias:.2f}%\" if np.isfinite(pbias) else \"Test PBIAS: N/A\")\n",
    "    print(f\"Test MAE: {mae:.4f}\" if np.isfinite(mae) else \"Test MAE: N/A\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\" if np.isfinite(rmse) else \"Test RMSE: N/A\")\n",
    "    print(f\"Test PCC: {pcc:.4f}\" if np.isfinite(pcc) else \"Test PCC: N/A\")\n",
    "    print(f\"Test Moran's I: {morans_i:.4f}\" if np.isfinite(morans_i) else \"Test Moran's I: N/A\")\n",
    "    \n",
    "    # Additional diagnostic information\n",
    "    print(f\"\\nDiagnostic Information:\")\n",
    "    print(f\"Predictions - Min: {y_pred_flat.min():.3f}, Max: {y_pred_flat.max():.3f}, Mean: {y_pred_flat.mean():.3f}\")\n",
    "    print(f\"True values - Min: {y_test_flat.min():.3f}, Max: {y_test_flat.max():.3f}, Mean: {y_test_flat.mean():.3f}\")\n",
    "    print(f\"Prediction std: {np.std(y_pred_flat):.3f}\")\n",
    "    print(f\"True values std: {np.std(y_test_flat):.3f}\")\n",
    "    \n",
    "    # Check for prediction issues\n",
    "    nan_predictions = np.sum(np.isnan(y_pred_flat))\n",
    "    inf_predictions = np.sum(np.isinf(y_pred_flat))\n",
    "    if nan_predictions > 0 or inf_predictions > 0:\n",
    "        print(f\"WARNING: {nan_predictions} NaN predictions, {inf_predictions} Inf predictions\")\n",
    "    \n",
    "    # Save results for comparison\n",
    "    results_dict = {\n",
    "        'target': target_metal,\n",
    "        'best_params': best_params,\n",
    "        'mape': mape,\n",
    "        'nse': nse,\n",
    "        'pbias': pbias,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'pcc': pcc,\n",
    "        'morans_i': morans_i,\n",
    "        'selected_features': selected_features,\n",
    "        'n_train': len(X_train_seq),\n",
    "        'n_val': len(X_val_seq),\n",
    "        'n_test': len(X_test_seq)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed successfully!\")\n",
    "    \n",
    "    \n",
    "def compare_models_for_multiple_targets():\n",
    "    \"\"\"Compare model performance across different heavy metals.\"\"\"\n",
    "    targets = ['Zn(mg/kg)', 'As(mg/kg)', 'Pb(mg/kg)', 'Cu(mg/kg)', 'Cr(mg/kg)']\n",
    "    all_results = []\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training model for {target}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Tune hyperparameters for this target\n",
    "            best_params, best_model, tuning_results = improved_tune_hyperparameters(\n",
    "                target=target, \n",
    "                file_path=\"gaussian_synthetic_complete.csv\",\n",
    "                max_epochs=30  # Reduced for multiple targets\n",
    "            )\n",
    "            \n",
    "            if best_model is None:\n",
    "                print(f\"Failed to train model for {target}\")\n",
    "                continue\n",
    "            \n",
    "            # Load data with best parameters\n",
    "            X_seq, y_seq, selected_features, y_mean, y_std, coords, cluster_labels, scaler = \\\n",
    "                ImprovedElmanNetwork.load_and_preprocess_data(\n",
    "                    target=target, \n",
    "                    k=best_params['k'], \n",
    "                    file_path=\"gaussian_synthetic_complete.csv\"\n",
    "                )\n",
    "            \n",
    "            # Split data\n",
    "            unique_clusters = np.unique(cluster_labels)\n",
    "            n_train = max(1, int(0.7 * len(unique_clusters)))\n",
    "            n_val = max(1, int(0.2 * len(unique_clusters)))\n",
    "            \n",
    "            train_clusters = unique_clusters[:n_train]\n",
    "            val_clusters = unique_clusters[n_train:n_train+n_val]\n",
    "            test_clusters = unique_clusters[n_train+n_val:]\n",
    "            \n",
    "            train_idx = np.isin(cluster_labels, train_clusters)\n",
    "            val_idx = np.isin(cluster_labels, val_clusters)\n",
    "            test_idx = np.isin(cluster_labels, test_clusters)\n",
    "            \n",
    "            X_test_seq = X_seq[test_idx]\n",
    "            y_test_seq = y_seq[test_idx]\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred_norm = best_model.predict(X_test_seq)\n",
    "            y_pred = y_pred_norm * y_std + y_mean\n",
    "            y_test_denorm = y_test_seq * y_std + y_mean\n",
    "            \n",
    "            # Apply target-specific clipping\n",
    "            if target == 'Zn(mg/kg)':\n",
    "                y_pred = np.clip(y_pred, 0, 200)\n",
    "            elif target == 'As(mg/kg)':\n",
    "                y_pred = np.clip(y_pred, 0, 50)\n",
    "            elif target == 'Pb(mg/kg)':\n",
    "                y_pred = np.clip(y_pred, 0, 100)\n",
    "            elif target == 'Cu(mg/kg)':\n",
    "                y_pred = np.clip(y_pred, 0, 150)\n",
    "            elif target == 'Cr(mg/kg)':\n",
    "                y_pred = np.clip(y_pred, 0, 100)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            y_test_flat = y_test_denorm.flatten()\n",
    "            y_pred_flat = y_pred.flatten()\n",
    "            \n",
    "            results = {\n",
    "                'target': target,\n",
    "                'k': best_params['k'],\n",
    "                'n_hidden': best_params['n_hidden'],\n",
    "                'mape': safe_mape(y_test_flat, y_pred_flat),\n",
    "                'nse': calculate_nse(y_test_flat, y_pred_flat),\n",
    "                'pbias': calculate_pbias(y_test_flat, y_pred_flat),\n",
    "                'mae': calculate_mae(y_test_flat, y_pred_flat),\n",
    "                'rmse': calculate_rmse(y_test_flat, y_pred_flat),\n",
    "                'pcc': calculate_pcc(y_test_flat, y_pred_flat),\n",
    "                'morans_i': calculate_morans_i(y_pred_flat, coords[test_idx], k=3),\n",
    "                'n_features': len(selected_features),\n",
    "                'n_test': len(y_test_flat)\n",
    "            }\n",
    "            \n",
    "            all_results.append(results)\n",
    "            \n",
    "            print(f\"\\nResults for {target}:\")\n",
    "            for metric, value in results.items():\n",
    "                if metric not in ['target', 'k', 'n_hidden', 'n_features', 'n_test']:\n",
    "                    if np.isfinite(value):\n",
    "                        if 'bias' in metric.lower() or 'mape' in metric.lower():\n",
    "                            print(f\"  {metric}: {value:.2f}%\")\n",
    "                        else:\n",
    "                            print(f\"  {metric}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric}: N/A\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model for {target}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary table\n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY TABLE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'Target':<12} {'MAPE%':<8} {'NSE':<8} {'PBIAS%':<8} {'MAE':<8} {'RMSE':<8} {'PCC':<8}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        for result in all_results:\n",
    "            mape_str = f\"{result['mape']:.1f}\" if np.isfinite(result['mape']) else \"N/A\"\n",
    "            nse_str = f\"{result['nse']:.3f}\" if np.isfinite(result['nse']) else \"N/A\"\n",
    "            pbias_str = f\"{result['pbias']:.1f}\" if np.isfinite(result['pbias']) else \"N/A\"\n",
    "            mae_str = f\"{result['mae']:.2f}\" if np.isfinite(result['mae']) else \"N/A\"\n",
    "            rmse_str = f\"{result['rmse']:.2f}\" if np.isfinite(result['rmse']) else \"N/A\"\n",
    "            pcc_str = f\"{result['pcc']:.3f}\" if np.isfinite(result['pcc']) else \"N/A\"\n",
    "            \n",
    "            print(f\"{result['target']:<12} {mape_str:<8} {nse_str:<8} {pbias_str:<8} {mae_str:<8} {rmse_str:<8} {pcc_str:<8}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def debug_training_issues():\n",
    "    \"\"\"Debug common training issues.\"\"\"\n",
    "    print(\"=== Training Debug Mode ===\")\n",
    "    \n",
    "    # Load data for Zn\n",
    "    target = 'As(mg/kg)'\n",
    "    X_seq, y_seq, selected_features, y_mean, y_std, coords, cluster_labels, scaler = \\\n",
    "        ImprovedElmanNetwork.load_and_preprocess_data(\n",
    "            target=target, \n",
    "            k=3, \n",
    "            file_path=\"/kaggle/input/private-new-dataset/gaussian_data.csv\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Data loaded successfully:\")\n",
    "    print(f\"  X_seq shape: {X_seq.shape}\")\n",
    "    print(f\"  y_seq shape: {y_seq.shape}\")\n",
    "    print(f\"  Selected features: {len(selected_features)}\")\n",
    "    print(f\"  Y statistics - Mean: {y_mean:.3f}, Std: {y_std:.3f}\")\n",
    "    \n",
    "    # Check for data issues\n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"  X_seq - NaN: {np.sum(np.isnan(X_seq))}, Inf: {np.sum(np.isinf(X_seq))}\")\n",
    "    print(f\"  y_seq - NaN: {np.sum(np.isnan(y_seq))}, Inf: {np.sum(np.isinf(y_seq))}\")\n",
    "    print(f\"  X_seq range: [{np.min(X_seq):.3f}, {np.max(X_seq):.3f}]\")\n",
    "    print(f\"  y_seq range: [{np.min(y_seq):.3f}, {np.max(y_seq):.3f}]\")\n",
    "    \n",
    "    # Simple split for testing\n",
    "    n_train = int(0.8 * len(X_seq))\n",
    "    X_train = X_seq[:n_train]\n",
    "    y_train = y_seq[:n_train]\n",
    "    X_test = X_seq[n_train:]\n",
    "    y_test = y_seq[n_train:]\n",
    "    \n",
    "    print(f\"\\nSimple split - Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Test very simple model\n",
    "    n_inputs = X_train.shape[2]\n",
    "    rnn = ImprovedElmanNetwork(n_inputs, 6, 1, lr=0.01, l2_lambda=0.001)\n",
    "    \n",
    "    print(f\"\\nTesting simple model (6 hidden units):\")\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        train_loss = rnn.train_epoch(X_train, y_train)\n",
    "        test_pred = rnn.predict(X_test)\n",
    "        test_loss = np.mean((test_pred - y_test) ** 2)\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}: Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "        \n",
    "        if not np.isfinite(train_loss) or not np.isfinite(test_loss):\n",
    "            print(\"  ERROR: Non-finite loss detected!\")\n",
    "            break\n",
    "        \n",
    "        # Check predictions\n",
    "        pred_stats = f\"Pred range: [{np.min(test_pred):.3f}, {np.max(test_pred):.3f}]\"\n",
    "        print(f\"    {pred_stats}\")\n",
    "    \n",
    "    return X_seq, y_seq, rnn\n",
    "\n",
    "\n",
    "# Additional utility function for model analysis\n",
    "def analyze_model_weights(model):\n",
    "    \"\"\"Analyze model weights for debugging.\"\"\"\n",
    "    print(\"=== Weight Analysis ===\")\n",
    "    \n",
    "    weights = {\n",
    "        'W_xh': model.W_xh,\n",
    "        'W_ch': model.W_ch,\n",
    "        'W_hy': model.W_hy,\n",
    "        'b_h': model.b_h,\n",
    "        'b_y': model.b_y\n",
    "    }\n",
    "    \n",
    "    for name, weight in weights.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {weight.shape}\")\n",
    "        print(f\"  Range: [{np.min(weight):.6f}, {np.max(weight):.6f}]\")\n",
    "        print(f\"  Mean: {np.mean(weight):.6f}, Std: {np.std(weight):.6f}\")\n",
    "        print(f\"  NaN count: {np.sum(np.isnan(weight))}\")\n",
    "        print(f\"  Inf count: {np.sum(np.isinf(weight))}\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90765ec1",
   "metadata": {},
   "source": [
    "Boosted tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b54764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "def load_and_preprocess_data():\n",
    "    data = pd.read_csv('gaussian_synthetic_complete.csv', encoding='latin-1')\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    X = data.drop(['As(mg/kg)', 'Zn(mg/kg)'], axis=1)\n",
    "    y_as = data['As(mg/kg)'].values\n",
    "    y_zn = data['Zn(mg/kg)'].values\n",
    "    \n",
    "    # Ensure numeric features (replace any non-numeric or NaN with 0)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    return X.values, y_as, y_zn, X.columns\n",
    "\n",
    "# Metrics calculation\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    pcc = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    \n",
    "    # Safe MAPE\n",
    "    nonzero_idx = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[nonzero_idx] - y_pred[nonzero_idx]) / y_true[nonzero_idx])) * 100\n",
    "    pbias = np.sum(y_pred - y_true) / np.sum(y_true) * 100\n",
    "    nse = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "\n",
    "    \n",
    "    return mae, rmse, pcc, mape, pbias,nse\n",
    "\n",
    "# Training and evaluation\n",
    "def train_and_evaluate_model(X, y, target_name, n_folds=10):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [20, 50, 100, 200, 300, 400, 500],\n",
    "        'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5],\n",
    "        'estimator__max_depth': [2, 3, 4, 5, 6, 7, 8, 10],\n",
    "    }\n",
    "    \n",
    "    model = AdaBoostRegressor(\n",
    "        estimator=DecisionTreeRegressor(random_state=42),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    metrics_results = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        metrics = calculate_metrics(y_test, y_pred)\n",
    "        metrics_results.append(metrics)\n",
    "        \n",
    "        print(f\"\\nFold {fold+1} - {target_name}:\")\n",
    "        print(f\"MAE: {metrics[0]:.4f}, RMSE: {metrics[1]:.4f}, PCC: {metrics[2]:.4f}, MAPE: {metrics[3]:.4f}, PBIAS: {metrics[4]:.4f}%,NSE: {metrics[5]:.4f}\")\n",
    "    \n",
    "    avg_metrics = np.mean(metrics_results, axis=0)\n",
    "    print(f\"\\nAverage {target_name} Results:\")\n",
    "    print(f\"MAE: {avg_metrics[0]:.4f}, RMSE: {avg_metrics[1]:.4f}, PCC: {avg_metrics[2]:.4f}, MAPE: {avg_metrics[3]:.4f}, PBIAS: {avg_metrics[4]:.4f}%,NSE: {metrics[5]:.4f}\")\n",
    "    \n",
    "    return best_model, avg_metrics\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    X, y_as, y_zn, feature_names = load_and_preprocess_data()\n",
    "    \n",
    "    print(\"\\n=== Predicting Arsenic (As) ===\")\n",
    "    model_as, avg_metrics_as = train_and_evaluate_model(X, y_as, \"Arsenic\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n=== Predicting Zinc (Zn) ===\")\n",
    "    model_zn, avg_metrics_zn = train_and_evaluate_model(X, y_zn, \"Zinc\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b46a24",
   "metadata": {},
   "source": [
    "Relevance Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f47952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BaseRVM:\n",
    "    def __init__(self, kernel='rbf', degree=3, coef1=None, coef0=0.0, n_iter=3000, \n",
    "                 tol=1e-3, alpha=1e-6, threshold_alpha=1e9, beta=1e-6, \n",
    "                 beta_fixed=False, bias_used=True, verbose=False):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.coef1 = coef1\n",
    "        self.coef0 = coef0\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.threshold_alpha = threshold_alpha\n",
    "        self.beta = beta\n",
    "        self.beta_fixed = beta_fixed\n",
    "        self.bias_used = bias_used\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "            'kernel': self.kernel, 'degree': self.degree, 'coef1': self.coef1, \n",
    "            'coef0': self.coef0, 'n_iter': self.n_iter, 'tol': self.tol, \n",
    "            'alpha': self.alpha, 'threshold_alpha': self.threshold_alpha, \n",
    "            'beta': self.beta, 'beta_fixed': self.beta_fixed, \n",
    "            'bias_used': self.bias_used, 'verbose': self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _apply_kernel(self, x, y):\n",
    "        from sklearn.metrics.pairwise import linear_kernel, rbf_kernel, polynomial_kernel\n",
    "        if self.kernel == 'linear':\n",
    "            phi = linear_kernel(x, y)\n",
    "        elif self.kernel == 'rbf':\n",
    "            phi = rbf_kernel(x, y, self.coef1)\n",
    "        elif self.kernel == 'poly':\n",
    "            phi = polynomial_kernel(x, y, self.degree, self.coef1, self.coef0)\n",
    "        elif callable(self.kernel):\n",
    "            phi = self.kernel(x, y)\n",
    "            if len(phi.shape) != 2 or phi.shape[0] != x.shape[0]:\n",
    "                raise ValueError(\"Invalid custom kernel output.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel selection.\")\n",
    "        if self.bias_used:\n",
    "            phi = np.append(phi, np.ones((phi.shape[0], 1)), axis=1)\n",
    "        return phi\n",
    "\n",
    "    def _prune(self):\n",
    "        keep_alpha = self.alpha_ < self.threshold_alpha\n",
    "        if not np.any(keep_alpha):\n",
    "            keep_alpha[0] = True\n",
    "            if self.bias_used:\n",
    "                keep_alpha[-1] = True\n",
    "        if self.bias_used:\n",
    "            if not keep_alpha[-1]:\n",
    "                self.bias_used = False\n",
    "            self.relevance_ = self.relevance_[keep_alpha[:-1]]\n",
    "        else:\n",
    "            self.relevance_ = self.relevance_[keep_alpha]\n",
    "        self.alpha_ = self.alpha_[keep_alpha]\n",
    "        self.alpha_old = self.alpha_old[keep_alpha]\n",
    "        self.gamma = self.gamma[keep_alpha]\n",
    "        self.phi = self.phi[:, keep_alpha]\n",
    "        self.sigma_ = self.sigma_[np.ix_(keep_alpha, keep_alpha)]\n",
    "        self.m_ = self.m_[keep_alpha]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        from sklearn.utils.validation import check_X_y\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.phi = self._apply_kernel(X, X)\n",
    "        n_basis_functions = self.phi.shape[1]\n",
    "        self.relevance_ = X\n",
    "        self.y = y\n",
    "        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n",
    "        self.beta_ = self.beta\n",
    "        self.m_ = np.zeros(n_basis_functions)\n",
    "        self.alpha_old = self.alpha_\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            self._posterior()\n",
    "            self.gamma = 1 - self.alpha_*np.diag(self.sigma_)\n",
    "            self.alpha_ = self.gamma/(self.m_ ** 2 + 1e-10)  # Added small constant for stability\n",
    "            \n",
    "            if not self.beta_fixed:\n",
    "                residual_sum = np.sum((y - np.dot(self.phi, self.m_)) ** 2)\n",
    "                self.beta_ = max((n_samples - np.sum(self.gamma))/residual_sum, 1e-6)\n",
    "            \n",
    "            self._prune()\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration: {i}, Relevance Vectors: {self.relevance_.shape[0]}, Beta: {self.beta_:.6f}\")\n",
    "            \n",
    "            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n",
    "            if delta < self.tol and i > 10:\n",
    "                break\n",
    "            self.alpha_old = self.alpha_.copy()\n",
    "        \n",
    "        if self.bias_used:\n",
    "            self.bias = self.m_[-1] if len(self.m_) > 0 else 0\n",
    "        else:\n",
    "            self.bias = None\n",
    "        return self\n",
    "\n",
    "class RVR(BaseRVM):\n",
    "    def _posterior(self):\n",
    "        try:\n",
    "            i_s = np.diag(self.alpha_) + self.beta_ * np.dot(self.phi.T, self.phi)\n",
    "            self.sigma_ = np.linalg.inv(i_s)\n",
    "            self.m_ = self.beta_ * np.dot(self.sigma_, np.dot(self.phi.T, self.y))\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Handle singular matrix\n",
    "            i_s = np.diag(self.alpha_) + self.beta_ * np.dot(self.phi.T, self.phi)\n",
    "            self.sigma_ = np.linalg.pinv(i_s)\n",
    "            self.m_ = self.beta_ * np.dot(self.sigma_, np.dot(self.phi.T, self.y))\n",
    "\n",
    "    def predict(self, X, eval_MSE=False):\n",
    "        phi = self._apply_kernel(X, self.relevance_)\n",
    "        y = np.dot(phi, self.m_)\n",
    "        if eval_MSE:\n",
    "            MSE = (1/self.beta_) + np.diag(np.dot(phi, np.dot(self.sigma_, phi.T)))\n",
    "            return y, MSE\n",
    "        return y\n",
    "\n",
    "class MarineSedimentPredictor:\n",
    "    \"\"\"Enhanced predictor class for marine sediment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features = [\n",
    "            'longitude (° E)', 'latitude (°N)', 'Cr(mg/kg)', 'Cu(mg/kg)', \n",
    "            'Cd(mg/kg)', 'Pb(mg/kg)', 'Hg(mg/kg)', 'Ni(mg/kg)', \n",
    "            'Al2O3 (%)', 'clay (%)', 'silt (%)', 'sand (%)', \n",
    "            'mean grain size(Mz, Φ)', 'water depth (m)'\n",
    "        ]\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.metrics_history = {}\n",
    "    \n",
    "    def load_data(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate the dataset\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "            \n",
    "            # Check for missing features\n",
    "            missing_features = [f for f in self.features if f not in df.columns]\n",
    "            if missing_features:\n",
    "                print(f\"Warning: Missing features: {missing_features}\")\n",
    "                self.features = [f for f in self.features if f in df.columns]\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        # Handle potential division by zero in MAPE\n",
    "        mask = y_true != 0\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if np.any(mask) else np.inf\n",
    "        \n",
    "        # RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        \n",
    "        # MAE\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        # PCC\n",
    "        pcc, _ = pearsonr(y_true, y_pred)\n",
    "        \n",
    "        # NSE\n",
    "        nse = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "        \n",
    "        # PBIAS\n",
    "        sum_y_true = np.sum(y_true)\n",
    "        pbias = (100 / sum_y_true * np.sum(y_true - y_pred)) if sum_y_true != 0 else np.inf\n",
    "        \n",
    "        return {\n",
    "            'RMSE': rmse, 'MAE': mae, 'MAPE': mape,\n",
    "            'PCC': pcc, 'NSE': nse, 'PBIAS': pbias\n",
    "        }\n",
    "    \n",
    "    def print_formatted_metrics(self, metrics: Dict[str, float], prefix: str = \"Test\"):\n",
    "        \"\"\"Print metrics in the requested format\"\"\"\n",
    "        print(f\"{prefix} MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"{prefix} NSE: {metrics['NSE']:.4f}\")\n",
    "        print(f\"{prefix} PBIAS: {metrics['PBIAS']:.2f}%\")\n",
    "        print(f\"{prefix} MAE: {metrics['MAE']:.4f}\")\n",
    "        print(f\"{prefix} RMSE: {metrics['RMSE']:.4f}\")\n",
    "        print(f\"{prefix} PCC: {metrics['PCC']:.4f}\")\n",
    "    \n",
    "    def kfold_cross_validation(self, X: np.ndarray, y: np.ndarray, \n",
    "                             target_name: str, n_folds: int = 10,\n",
    "                             kernel: str = 'rbf', **kwargs) -> Tuple[Dict, RVR, StandardScaler]:\n",
    "        \"\"\"Perform k-fold cross-validation\"\"\"\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        metrics_all = {key: [] for key in ['RMSE', 'MAE', 'MAPE', 'PCC', 'NSE', 'PBIAS']}\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating RVR for {target_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        best_model = None\n",
    "        best_scaler = None\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train RVR\n",
    "            model = RVR(kernel=kernel, verbose=False, **kwargs)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_metrics(y_test, y_pred)\n",
    "            for key in metrics_all:\n",
    "                metrics_all[key].append(metrics[key])\n",
    "            \n",
    "            # Track best model based on PCC\n",
    "            if metrics['PCC'] > best_score:\n",
    "                best_score = metrics['PCC']\n",
    "                best_model = model\n",
    "                best_scaler = scaler\n",
    "            \n",
    "            print(f\"Fold {fold+1}:  RMSE={metrics['RMSE']:.4f}, \"\n",
    "                  f\"MAE={metrics['MAE']:.4f}, PCC={metrics['PCC']:.4f}\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {key: np.mean(values) for key, values in metrics_all.items()}\n",
    "        std_metrics = {key: np.std(values) for key, values in metrics_all.items()}\n",
    "        \n",
    "        print(f\"\\n{target_name} - Cross-Validation Results:\")\n",
    "        self.print_formatted_metrics(avg_metrics, \"CV Average\")\n",
    "        \n",
    "        self.metrics_history[target_name] = {\n",
    "            'avg': avg_metrics,\n",
    "            'std': std_metrics,\n",
    "            'all_folds': metrics_all\n",
    "        }\n",
    "        \n",
    "        return avg_metrics, best_model, best_scaler\n",
    "    \n",
    "    def train_model(self, df: pd.DataFrame, target_column: str, \n",
    "                   test_size: float = 0.2, **kwargs) -> Tuple[RVR, StandardScaler]:\n",
    "        \"\"\"Train a single model with train-test split and formatted output\"\"\"\n",
    "        X = df[self.features].values\n",
    "        y = df[target_column].values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = RVR(verbose=True, **kwargs)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Test Set Performance for {target_column}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        self.print_formatted_metrics(metrics)\n",
    "        \n",
    "        self.models[target_column] = model\n",
    "        self.scalers[target_column] = scaler\n",
    "        \n",
    "        return model, scaler\n",
    "    \n",
    "    def predict_new_samples(self, new_data: np.ndarray, target: str, \n",
    "                          with_uncertainty: bool = False) -> np.ndarray:\n",
    "        \"\"\"Make predictions on new samples\"\"\"\n",
    "        if target not in self.models:\n",
    "            raise ValueError(f\"Model for {target} not trained yet\")\n",
    "        \n",
    "        model = self.models[target]\n",
    "        scaler = self.scalers[target]\n",
    "        \n",
    "        # Scale new data\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "        \n",
    "        # Predict\n",
    "        if with_uncertainty:\n",
    "            predictions, uncertainty = model.predict(new_data_scaled, eval_MSE=True)\n",
    "            return predictions, uncertainty\n",
    "        else:\n",
    "            return model.predict(new_data_scaled)\n",
    "    \n",
    "    \n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize predictor\n",
    "    predictor = MarineSedimentPredictor()\n",
    "    \n",
    "    # Load data (replace with your actual file path)\n",
    "    try:\n",
    "        df = predictor.load_data('/kaggle/input/private-new-dataset/gaussian_data.csv')\n",
    "        if df is None:\n",
    "            print(\"Failed to load data. Creating synthetic example...\")\n",
    "            # Create synthetic data for demonstration\n",
    "            np.random.seed(42)\n",
    "            n_samples = 200\n",
    "            df = pd.DataFrame({\n",
    "                'longitude (° E)': np.random.uniform(110, 130, n_samples),\n",
    "                'latitude (°N)': np.random.uniform(20, 40, n_samples),\n",
    "                'Cr(mg/kg)': np.random.lognormal(3, 0.5, n_samples),\n",
    "                'Cu(mg/kg)': np.random.lognormal(2.5, 0.6, n_samples),\n",
    "                'Cd(mg/kg)': np.random.lognormal(0, 0.8, n_samples),\n",
    "                'Pb(mg/kg)': np.random.lognormal(2, 0.7, n_samples),\n",
    "                'Hg(mg/kg)': np.random.lognormal(-1, 0.9, n_samples),\n",
    "                'Ni(mg/kg)': np.random.lognormal(3.2, 0.4, n_samples),\n",
    "                'Al2O3 (%)': np.random.uniform(5, 20, n_samples),\n",
    "                'clay (%)': np.random.uniform(10, 60, n_samples),\n",
    "                'silt (%)': np.random.uniform(20, 70, n_samples),\n",
    "                'sand (%)': np.random.uniform(10, 70, n_samples),\n",
    "                'mean grain size(Mz, Φ)': np.random.uniform(2, 8, n_samples),\n",
    "                'water depth (m)': np.random.uniform(10, 200, n_samples)\n",
    "            })\n",
    "            # Create synthetic targets\n",
    "            df['As(mg/kg)'] = 2 + 0.3*df['Cu(mg/kg)'] + 0.2*df['clay (%)'] + np.random.normal(0, 2, n_samples)\n",
    "            df['Zn(mg/kg)'] = 50 + 0.8*df['Cu(mg/kg)'] + 0.4*df['Pb(mg/kg)'] + np.random.normal(0, 10, n_samples)\n",
    "            df[df < 0] = 0.1  # Ensure no negative values\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Train individual models with formatted output\n",
    "    print(\"Training models with train-test split...\")\n",
    "    \n",
    "    # Train Arsenic model\n",
    "    model_as, scaler_as = predictor.train_model(\n",
    "        df, 'As(mg/kg)', test_size=0.2, kernel='rbf'\n",
    "    )\n",
    "    \n",
    "    # Train Zinc model  \n",
    "    model_zn, scaler_zn = predictor.train_model(\n",
    "        df, 'Zn(mg/kg)', test_size=0.2, kernel='rbf'\n",
    "    )\n",
    "    \n",
    "    # Alternative: Perform cross-validation with formatted output\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X = df[predictor.features].values\n",
    "    y_as = df['As(mg/kg)'].values\n",
    "    y_zn = df['Zn(mg/kg)'].values\n",
    "    \n",
    "    # Cross-validation for Arsenic\n",
    "    metrics_as, _, _ = predictor.kfold_cross_validation(\n",
    "        X, y_as, 'Arsenic (As)', n_folds=5, kernel='rbf'\n",
    "    )\n",
    "    \n",
    "    # Cross-validation for Zinc\n",
    "    metrics_zn, _, _ = predictor.kfold_cross_validation(\n",
    "        X, y_zn, 'Zinc (Zn)', n_folds=5, kernel='rbf'\n",
    "    )\n",
    "    \n",
    "    # Example prediction on new data\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Example Predictions on New Samples\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Use first 5 samples as \"new\" data for demonstration\n",
    "    new_samples = X[:5]\n",
    "    \n",
    "    # Predict with uncertainty\n",
    "    as_pred, as_uncertainty = predictor.predict_new_samples(\n",
    "        new_samples, 'As(mg/kg)', with_uncertainty=True\n",
    "    )\n",
    "    zn_pred, zn_uncertainty = predictor.predict_new_samples(\n",
    "        new_samples, 'Zn(mg/kg)', with_uncertainty=True\n",
    "    )\n",
    "    \n",
    "    print(\"Sample\\tAs Pred (±σ)\\t\\tZn Pred (±σ)\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(len(new_samples)):\n",
    "        print(f\"{i+1}\\t{as_pred[i]:.2f} (±{np.sqrt(as_uncertainty[i]):.2f})\\t\\t\"\n",
    "              f\"{zn_pred[i]:.2f} (±{np.sqrt(zn_uncertainty[i]):.2f})\")\n",
    "    \n",
    "    return predictor, df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor, df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfbebe",
   "metadata": {},
   "source": [
    "RVM-FPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ee3e8",
   "metadata": {},
   "source": [
    "FPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your FPA functions (unchanged)\n",
    "def levy_flight(beta=1.5):\n",
    "    r1 = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)\n",
    "    r2 = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)\n",
    "    sig_num = gamma(1+beta)*np.sin((np.pi*beta)/2.0)\n",
    "    sig_den = gamma((1+beta)/2)*beta*2**((beta-1)/2)\n",
    "    sigma = (sig_num/sig_den)**(1/beta)\n",
    "    levy = (0.01*r1*sigma)/(abs(r2)**(1/beta))\n",
    "    return levy\n",
    "\n",
    "def pollination_global(position, best_global, flower=0, gama=0.5, lamb=1.4, min_values=[-5], max_values=[5], target_function=None):\n",
    "    x = np.copy(best_global)\n",
    "    for j in range(0, len(min_values)):\n",
    "        x[j] = np.clip((position[flower, j] + gama*levy_flight(lamb)*(position[flower, j] - best_global[j])), min_values[j], max_values[j])\n",
    "    x[-1] = target_function(x[0:len(min_values)])\n",
    "    return x\n",
    "\n",
    "def pollination_local(position, best_global, flower=0, nb_flower_1=0, nb_flower_2=1, min_values=[-5], max_values=[5], target_function=None):\n",
    "    x = np.copy(best_global)\n",
    "    for j in range(0, len(min_values)):\n",
    "        r = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)\n",
    "        x[j] = np.clip((position[flower, j] + r*(position[nb_flower_1, j] - position[nb_flower_2, j])), min_values[j], max_values[j])\n",
    "    x[-1] = target_function(x[0:len(min_values)])\n",
    "    return x\n",
    "\n",
    "def initial_position(flowers=3, min_values=[-5], max_values=[5], target_function=None):\n",
    "    position = np.zeros((flowers, len(min_values)+1))\n",
    "    for i in range(0, flowers):\n",
    "        for j in range(0, len(min_values)):\n",
    "            position[i,j] = random.uniform(min_values[j], max_values[j])\n",
    "        position[i,-1] = target_function(position[i,0:position.shape[1]-1])\n",
    "    return position\n",
    "\n",
    "def flower_pollination_algorithm(flowers=3, min_values=[-5], max_values=[5], iterations=50, gama=0.5, lamb=1.4, p=0.8, target_function=None):    \n",
    "    count = 0\n",
    "    position = initial_position(flowers=flowers, min_values=min_values, max_values=max_values, target_function=target_function)\n",
    "    best_global = np.copy(position[position[:,-1].argsort()][0,:])\n",
    "    x = np.copy(best_global)   \n",
    "    while count <= iterations:\n",
    "        print(f\"Iteration = {count}, f(x) = {best_global[-1]}\")\n",
    "        for i in range(0, position.shape[0]):\n",
    "            nb_flower_1 = np.random.randint(position.shape[0])\n",
    "            nb_flower_2 = np.random.randint(position.shape[0])\n",
    "            while nb_flower_1 == nb_flower_2:\n",
    "                nb_flower_1 = np.random.randint(position.shape[0])\n",
    "            r = int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)\n",
    "            if r < p:\n",
    "                x = pollination_global(position, best_global, flower=i, gama=gama, lamb=lamb, min_values=min_values, max_values=max_values, target_function=target_function)\n",
    "            else:\n",
    "                x = pollination_local(position, best_global, flower=i, nb_flower_1=nb_flower_1, nb_flower_2=nb_flower_2, min_values=min_values, max_values=max_values, target_function=target_function)\n",
    "            if x[-1] <= position[i,-1]:\n",
    "                for j in range(0, position.shape[1]):\n",
    "                    position[i,j] = x[j]\n",
    "            value = np.copy(position[position[:,-1].argsort()][0,:])\n",
    "            if best_global[-1] > value[-1]:\n",
    "                best_global = np.copy(value) \n",
    "        count += 1       \n",
    "    return best_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('gaussian_synthetic_complete.csv', encoding=\"latin-1\")\n",
    "\n",
    "# Define features and targets using the suggested structure to avoid KeyError\n",
    "targets = ['Zn(mg/kg)', 'As(mg/kg)]']\n",
    "drop_cols = ['station', 'sample number']\n",
    "\n",
    "numerical_features = [col for col in df.columns if col not in targets + drop_cols]\n",
    "\n",
    "# Preprocessor to handle numerical scaling and categorical encoding (avoids manual drop and normalizes)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transform to get X\n",
    "X = preprocessor.fit_transform(df)\n",
    "\n",
    "# Define targets\n",
    "y_as = df['As(mg/kg)'].values\n",
    "y_zn = df['Zn(mg/kg)'].values\n",
    "\n",
    "# Split data (same splits for both targets)\n",
    "X_train, X_test, y_as_train, y_as_test = train_test_split(X, y_as, test_size=0.2, random_state=42)\n",
    "_, _, y_zn_train, y_zn_test = train_test_split(X, y_zn, test_size=0.2, random_state=42)\n",
    "# Objective function for FPA (RVM RMSE)\n",
    "def rvm_objective(params, X_train, X_test, y_train, y_test):\n",
    "    gamma = params[0]\n",
    "    model = RVR(kernel='rbf', coef1=gamma)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Optimize RVM for As\n",
    "def objective_as(params):\n",
    "    return rvm_objective(params, X_train, X_test, y_as_train, y_as_test)\n",
    "\n",
    "best_params_as = flower_pollination_algorithm(\n",
    "    flowers=15, min_values=[0.001], max_values=[10.0], iterations=30, \n",
    "    gama=0.5, lamb=1.4, p=0.8, target_function=objective_as\n",
    ")\n",
    "print(f\"Optimal gamma for As: {best_params_as[0]:.4f}, RMSE: {best_params_as[-1]:.4f}\")\n",
    "\n",
    "# Train final RVM for As\n",
    "rvm_as = RVR(kernel='rbf', coef1=best_params_as[0])\n",
    "rvm_as.fit(X_train, y_as_train)\n",
    "y_as_pred = rvm_as.predict(X_test)\n",
    "rmse_as = np.sqrt(mean_squared_error(y_as_test, y_as_pred))\n",
    "\n",
    "print(f\"As - Final Test RMSE: {rmse_as:.4f}, R²: {:.4f}\")\n",
    "\n",
    "# Optimize RVM for Zn\n",
    "def objective_zn(params):\n",
    "    return rvm_objective(params, X_train, X_test, y_zn_train, y_zn_test)\n",
    "\n",
    "best_params_zn = flower_pollination_algorithm(\n",
    "    flowers=15, min_values=[0.001], max_values=[10.0], iterations=30, \n",
    "    gama=0.5, lamb=1.4, p=0.8, target_function=objective_zn\n",
    ")\n",
    "print(f\"Optimal gamma for Zn: {best_params_zn[0]:.4f}, RMSE: {best_params_zn[-1]:.4f}\")\n",
    "\n",
    "# Train final RVM for Zn\n",
    "rvm_zn = RVR(kernel='rbf', coef1=best_params_zn[0])\n",
    "rvm_zn.fit(X_train, y_zn_train)\n",
    "y_zn_pred = rvm_zn.predict(X_test)\n",
    "rmse_zn = np.sqrt(mean_squared_error(y_zn_test, y_zn_pred))\n",
    "r2_zn = r2_score(y_zn_test, y_zn_pred)\n",
    "print(f\"Zn - Final Test RMSE: {rmse_zn:.4f}, R²: {r2_zn:.4f}\")\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\nSample Predictions (As, Zn) vs Actual:\")\n",
    "print(np.column_stack((y_as_pred[:5], y_as_test[:5], y_zn_pred[:5], y_zn_test[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd49365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new Actual data.csv')\n",
    "five_samp = data.head(11)\n",
    "five_samp.to_csv('samples.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
